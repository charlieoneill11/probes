{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple, Optional, Any, Union\n",
    "from dataclasses import dataclass\n",
    "from openai import OpenAI\n",
    "import yaml\n",
    "\n",
    "### OUR IMPORTS ###\n",
    "from data import ConceptExampleGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config\n",
    "with open(\"config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Get API key from environment variable\n",
    "api_key = config[\"openai_key\"]\n",
    "\n",
    "# Initialize the generator\n",
    "generator = ConceptExampleGenerator(api_key)\n",
    "\n",
    "# # Generate examples for the concept \"irony\"\n",
    "# examples = generator.generate_examples(\n",
    "#     concept=\"femur fracture\",\n",
    "#     k=5,\n",
    "#     domain=\"clinical medicine\",\n",
    "#     example_length=\"medium\"\n",
    "# )\n",
    "\n",
    "# # Print the examples\n",
    "# for i, example in enumerate(examples):\n",
    "#     print(f\"\\nExample {i+1}:\")\n",
    "#     print(f\"Positive: {example['positive']}\")\n",
    "#     print(f\"Negative: {example['negative']}\")\n",
    "# print(\"\\n\\n\\n\")\n",
    "\n",
    "# # Format for probe training\n",
    "# texts, labels = generator.format_examples_for_probe(examples)\n",
    "# print(f\"\\nGenerated {len(texts)} examples for probe training\")\n",
    "\n",
    "# Generate a larger dataset in batches\n",
    "# large_examples = generator.generate_examples_batch(\n",
    "#     concept=\"femur fracture\",\n",
    "#     k=50,\n",
    "#     batch_size=5\n",
    "# )\n",
    "print(f\"Generated {len(large_examples)} total examples in batches\")\n",
    "\n",
    "# Print large examples\n",
    "for i, example in enumerate(large_examples):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Positive: {example['positive']}\")\n",
    "    print(f\"Negative: {example['negative']}\")\n",
    "\n",
    "# Save examples to file\n",
    "generator.save_examples_to_file(large_examples, \"femur_examples.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformer_lens as tl\n",
    "import transformer_lens.utils as utils\n",
    "import json\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load gpt2-small\n",
    "model = tl.HookedTransformer.from_pretrained(\"gpt2-small\", device=device)\n",
    "\n",
    "# load examples\n",
    "with open(\"femur_examples.json\", \"r\") as f:\n",
    "    large_examples = json.load(f)[\"examples\"]\n",
    "\n",
    "print(large_examples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack positive examples\n",
    "pos_examples = [x[\"positive\"] for x in large_examples]\n",
    "neg_examples = [x[\"negative\"] for x in large_examples]\n",
    "\n",
    "_, pos_cache = model.run_with_cache(model.to_tokens(pos_examples), stop_at_layer=layer+1, names_filter=[hook_name])\n",
    "_, neg_cache = model.run_with_cache(model.to_tokens(neg_examples), stop_at_layer=layer+1, names_filter=[hook_name])\n",
    "\n",
    "pos_resid = pos_cache[hook_name][:, -1] # batch, seq, d_model -> batch, d_model\n",
    "neg_resid = neg_cache[hook_name][:, -1] # batch, seq, d_model -> batch, d_model\n",
    "\n",
    "print(pos_resid.shape, neg_resid.shape)\n",
    "\n",
    "# stack and create labels\n",
    "resid = torch.cat([pos_resid, neg_resid], dim=0)\n",
    "labels = torch.cat([torch.ones(len(pos_resid)), torch.zeros(len(neg_resid))])\n",
    "\n",
    "# Shuffle and split into train/val\n",
    "indices = torch.randperm(len(resid))\n",
    "resid = resid[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "train_size = int(0.8 * len(resid))\n",
    "train_resid = resid[:train_size]\n",
    "train_labels = labels[:train_size] \n",
    "val_resid = resid[train_size:]\n",
    "val_labels = labels[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = pos_resid.shape[1]\n",
    "\n",
    "linear_probe = nn.Linear(d_model, 1, bias=True)\n",
    "nn.init.xavier_normal_(linear_probe.weight)\n",
    "nn.init.zeros_(linear_probe.bias)\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(linear_probe.parameters(), lr=1e-3)\n",
    "\n",
    "@torch.no_grad()\n",
    "def accuracy(logits, labels):\n",
    "    preds = torch.round(torch.sigmoid(logits))\n",
    "    print(preds, labels)\n",
    "    return (preds == labels).float().mean()\n",
    "\n",
    "# dictionary to store results\n",
    "results = {\n",
    "    \"train_loss\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"train_acc\": [],\n",
    "    \"val_acc\": []\n",
    "}\n",
    "\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    logits = linear_probe(train_resid)\n",
    "    loss = loss_fn(logits.squeeze(), train_labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_acc = accuracy(logits, train_labels)\n",
    "    val_logits = linear_probe(val_resid)\n",
    "    val_loss = loss_fn(val_logits.squeeze(), val_labels)\n",
    "    val_acc = accuracy(val_logits, val_labels)  \n",
    "    #print(f\"Epoch {epoch+1}, Train Loss: {loss.item()}, Val Loss: {val_loss.item()}, Train Acc: {train_acc.item()}, Val Acc: {val_acc.item()}\")\n",
    "    results[\"train_loss\"].append(loss.item())\n",
    "    results[\"val_loss\"].append(val_loss.item())\n",
    "    results[\"train_acc\"].append(train_acc.item())\n",
    "    results[\"val_acc\"].append(val_acc.item())\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "train_loss = results[\"train_loss\"]\n",
    "val_loss = results[\"val_loss\"]\n",
    "train_acc = results[\"train_acc\"]\n",
    "val_acc = results[\"val_acc\"]\n",
    "\n",
    "fig = px.line(data_frame=pd.DataFrame({\n",
    "    'epoch': range(len(train_loss)),\n",
    "    'Train Loss': train_loss,\n",
    "    'Validation Loss': val_loss\n",
    "}).melt(id_vars=['epoch'], var_name='Metric', value_name='Loss'),\n",
    "    x='epoch', y='Loss', color='Metric')\n",
    "fig.show()\n",
    "\n",
    "# Now plot accuracy\n",
    "fig = px.line(data_frame=pd.DataFrame({\n",
    "    'epoch': range(len(train_acc)),\n",
    "    'Train Accuracy': train_acc,\n",
    "    'Validation Accuracy': val_acc\n",
    "}).melt(id_vars=['epoch'], var_name='Metric', value_name='Accuracy'),\n",
    "    x='epoch', y='Accuracy', color='Metric')\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
